<!DOCTYPE HTML>
<html>
	<head>
		<title>Evaluation Framework</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
				<header id="header">
					<h1>TWR-CLOUD-Bern</h1>
					<nav id="naiv">
						<ul>
							<li class="special">
								<a href="#menu" class="menuToggle"><span>Menu</span></a>
								<div id="menu">
									<ul>
										<li><a href="index.html"><img src="content/images/home.png" alt="Home TWR-CLOUD-Bern" style="width:15px; vertical-align:middle; margin-right:5px;"> Home</a></li>
										<li><a class="icon fa-lightbulb" href="methodology.html">Methodology</a></li>
										<li><a class="icon fa-list-alt" href="dataset.html">Dataset</a></li>
										<li><a class="icon fa-chart-bar" href="evaluation_framework.html">Evaluation Framework</a></li>
									</ul>
								</div>
							</li>
						</ul>
					</nav>
				</header>

				<!-- Main -->
					<article id="main">
						<section class="wrapper style5">
							<div class="inner">

								<h2>Evaluation Framework</h2>

								<p class="jtext">
									Instructions for evaluating positioning methods and developing ranging models using
									our measurements, including how to quantify performance for comparisons.
								</p>

								<div class="col-12"><span class="image fit"><img src="content/images/evaluation_framework.jpg" alt="" /></span></div>

								<hr />

								<h3>Overview</h3>

								<p class="jtext">
									The <a href="methodology.html">TWR-Cloud-Bern dataset</a> provides comprehensive
									ranging measurements to support the development of empirical UWB ranging models and
									the evaluation of indoor positioning systems. In this section we propose a
									standardized evaluation framework that uses the
									<code>Rangings at Responders.pkl</code> measurements to enable fair comparisons
									between different localization methods. This evaluation framework specifically
									supports three positioning modes: <u>1) pure anchor-based positioning</u> (using
									only fixed reference nodes), <u>2) cooperative positioning</u> (where nodes with
									unknown positions collaborate), and <u>3) hybrid positioning</u> (combining both
									anchors and cooperation). Each generated evaluation scenario contains the necessary
									ranging measurements and position information to implement any of these three
									approaches. Nevertheless, it's important to emphasize that researchers have complete
									freedom in how they utilize the dataset.
								</p>

								<h4>Dataset Flexibility</h4>

								<p class="jtext">
									The dataset consists of 12 scans - 11 training scans and 1 evaluation scan that is
									unique among all scans as it uses a completely distinct set of node positions (no
									position used in this scan was ever used in any of the training scans). This
									deliberate separation ensures that the evaluation scan represents entirely unseen
									deployment conditions, with no available training data for any of its positions,
									making it ideal for unbiased evaluation of positioning methods. However, researchers
									are welcome to utilize our dataset in various ways beyond the proposed evaluation
									framework. For instance, the training data (but also the evaluation data) can be
									used to develop custom ranging models suited for indoor environments. These might be
									probabilistic models that capture the statistical nature of UWB ranging errors,
									deterministic approaches that model specific environmental conditions, or any other
									formulation that captures UWB signal behavior in indoor spaces.
								</p>

								<p class="jtext">
									The dataset's flexibility extends to how nodes are utilized in positioning
									scenarios. While our framework assigns the 40 utilized nodes to specific anchor and
									agent groups of predefined sizes, researchers can implement and assess alternative
									schemes. For example, when evaluating an anchor-based positioning method, one could
									theoretically use up to 39 nodes as anchors to localize a single node. Conversely,
									for cooperative positioning research, all 39 nodes could be treated as neighbors
									with unknown positions, creating a cooperative scenario of maximal neighbor
									availability. These examples illustrate how the dataset supports research beyond the
									standardized evaluation framework.
								</p>

								<hr />

								<h3>Evaluation Framework Construction</h3>

								<p class="jtext">
									Using the <code>Rangings at Responders.pkl</code> measurements from the evaluation
									scan, our framework generates standardized test scenarios that enable direct
									comparisons between different positioning methods. The node positions used in these
									scenarios is visualized in the deployment map showing candidate anchor positions in
									yellow and candidate agent positions in green circles. This visual reference helps
									researchers understand the spatial distribution of nodes in the evaluation
									scenarios.
								</p>

								<div class="col-12">
									<span class="image fit">
										<img src="content/images/node_deployment.jpg" alt="Node Deployment Map" />
									</span>
								</div>

								<p class="jtext">
									The framework constructs evaluation scenarios through three phases:
								</p>

								<h4>Phase 1: Anchor Group Selection</h4>

								<p class="jtext">
									Starting with 40 sampling positions from the evaluation scan, the process aims to
									divide these into two disjoint supersets of 20 positions each for anchors and
									agents. The anchor selection follows a hierarchical approach, first identifying the
									optimal deployment of 20 anchors by evaluating combinations that maximize spatial
									dispersion through minimum pairwise distances. This 20-anchor configuration becomes
									the superset from which all smaller anchor configurations are derived - essentially,
									we first establish which 20 positions will serve as potential anchor positions
									throughout all evaluations. The remaining 20 positions automatically form the agent
									position superset.
								</p>

								<p class="jtext">
									Once this division is established, the process continues by selecting optimal
									subsets of 16, 12, 8, and 4 anchors (made from within the 20-anchor superset). For
									each of these smaller group sizes, the algorithm evaluates again all possible
									position combinations, computing the minimum distance between any pair of anchors
									and selecting configurations that maximize this metric. This systematic approach
									guarantees that we maintain optimal spatial coverage (reflecting typical real-world
									anchor deployment strategies that aim for maximum coverage) while ensuring all
									evaluated configurations draw from the same position superset, enabling fair
									comparisons across different scenarios regardless of the anchor group size used.
								</p>

								<h4>Phase 2: Agent Group Selection</h4>

								<p class="jtext">
									Using the 20 positions designated for agents, the framework generates multiple
									configurations for different group sizes (4, 7, 10, 13, 16, and 19 agents). Rather
									than optimizing for a single criterion, this phase creates up to 5,000 different
									configurations per size, with mean pairwise distances distributed uniformly across
									possible values. The algorithm first generates all possible combinations for each
									size, computes their mean pairwise distances, and then samples configurations to
									achieve uniform distribution across the range of possible mean distances. This
									approach ensures the evaluation framework tests positioning methods across a wide
									range of realistic deployment patterns, from tightly clustered groups to widely
									dispersed configurations.
								</p>

								<h4>Phase 3: Scenario Construction</h4>

								<hr />
							</div>
						</section>
					</article>

				<!-- CTA -->
				<section class="wrapper style4">
					<div class="inner" style="display: flex; align-items: center;">
						<div class="text-content" style="flex: 1; text-align: right; padding-right: 20px;">
							<header>
								<h2>Contact</h2>
								<p>firstname.lastname@unibe.ch</p>
								<a href="content/twrcloudbern2024.bib" download="twrcloudbern2024.bib">BibTeX entry</a>
							</header>
						</div>
						<div class="image-container" style="display: flex; justify-content: flex-start; gap: 20px;">
							<div style="text-align: center;">
								<img src="content/images/authors/Xenakis.png" alt="Dimitrios Xenakis" style="width: auto; height: 150px;">
							</div>
							<div style="text-align: center;">
								<img src="content/images/authors/DiMaio.png" alt="Antonio Di Maio" style="width: auto; height: 150px;">
							</div>
							<div style="text-align: center;">
								<img src="content/images/authors/Braun.png" alt="Torsten Braun" style="width: auto; height: 150px;">
							</div>
						</div>
					</div>
				</section>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>